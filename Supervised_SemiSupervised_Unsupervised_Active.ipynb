{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70436879",
   "metadata": {},
   "source": [
    "### Homework 08\n",
    "\n",
    "Name: Yining Liu  \n",
    "Github Username: Lynzz1701    \n",
    "USC ID: 6168529797 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f412a30",
   "metadata": {},
   "source": [
    "### 1. Supervised, Semi-Supervised, and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07966d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate\n",
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c36e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "WDBC = pd.read_csv('../data/wdbc.data',names=['ID','Diagnosis']+[i for i in range(1,31)])\n",
    "X = WDBC.iloc[:,2:]\n",
    "y = WDBC['Diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc3275",
   "metadata": {},
   "source": [
    "#### 1(b)i. Supervised Learning: Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuracy, precision, recall, F1-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8bffc568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- batch, 1 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9912087912087912 , precision = 0.9917973286875725 , recall = 0.989422084623323\n",
      "    f1 score = 0.9905863367401828 , AUC = 0.989422084623323\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9510317720275139 , recall = 0.9553571428571428\n",
      "    f1 score = 0.9531057178116 , AUC = 0.9553571428571429\n",
      "\n",
      "---- batch, 2 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.9870789779326365 , recall = 0.9847265221878225\n",
      "    f1 score = 0.9858795051102742 , AUC = 0.9847265221878225\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9811507936507937 , recall = 0.9811507936507937\n",
      "    f1 score = 0.9811507936507937 , AUC = 0.9811507936507938\n",
      "\n",
      "---- batch, 3 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.9870789779326365 , recall = 0.9847265221878225\n",
      "    f1 score = 0.9858795051102742 , AUC = 0.9847265221878225\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9587662337662337 , recall = 0.9672619047619048\n",
      "    f1 score = 0.9626596790042581 , AUC = 0.9672619047619049\n",
      "\n",
      "---- batch, 4 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.988337432776087 , recall = 0.9835397316821466\n",
      "    f1 score = 0.9858449477351916 , AUC = 0.9835397316821466\n",
      "  - test set report -\n",
      "    accuracy = 0.9912280701754386 , precision = 0.9931506849315068 , recall = 0.9880952380952381\n",
      "    f1 score = 0.9905276277523889 , AUC = 0.9880952380952381\n",
      "\n",
      "---- batch, 5 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.988337432776087 , recall = 0.9835397316821466\n",
      "    f1 score = 0.9858449477351916 , AUC = 0.9835397316821466\n",
      "  - test set report -\n",
      "    accuracy = 0.9298245614035088 , precision = 0.9412393162393162 , recall = 0.9097222222222223\n",
      "    f1 score = 0.9220512820512821 , AUC = 0.9097222222222224\n",
      "\n",
      "---- batch, 6 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.9896907216494846 , recall = 0.9823529411764707\n",
      "    f1 score = 0.9858096307385229 , AUC = 0.9823529411764707\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9864864864864865 , recall = 0.9761904761904762\n",
      "    f1 score = 0.9809555629802873 , AUC = 0.9761904761904762\n",
      "\n",
      "---- batch, 7 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9888380849919312 , recall = 0.9876676986584108\n",
      "    f1 score = 0.9882470850187788 , AUC = 0.9876676986584108\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9623015873015872 , recall = 0.9623015873015872\n",
      "    f1 score = 0.9623015873015872 , AUC = 0.9623015873015873\n",
      "\n",
      "---- batch, 8 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9846153846153847 , precision = 0.9853314204923487 , recall = 0.9817853457172343\n",
      "    f1 score = 0.9835060408801612 , AUC = 0.9817853457172343\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9675324675324675 , recall = 0.9404761904761905\n",
      "    f1 score = 0.951575906889814 , AUC = 0.9404761904761905\n",
      "\n",
      "---- batch, 9 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9846153846153847 , precision = 0.9853314204923487 , recall = 0.9817853457172343\n",
      "    f1 score = 0.9835060408801612 , AUC = 0.9817853457172343\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 10 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.9896907216494846 , recall = 0.9823529411764707\n",
      "    f1 score = 0.9858096307385229 , AUC = 0.9823529411764707\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 11 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9802197802197802 , precision = 0.9818704284221526 , recall = 0.9759029927760579\n",
      "    f1 score = 0.9787410772225827 , AUC = 0.9759029927760579\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.9741062479117941 , recall = 0.9692460317460317\n",
      "    f1 score = 0.9715828832571667 , AUC = 0.9692460317460317\n",
      "\n",
      "---- batch, 12 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9846153846153847 , precision = 0.9853314204923487 , recall = 0.9817853457172343\n",
      "    f1 score = 0.9835060408801612 , AUC = 0.9817853457172343\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 13 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9824175824175824 , precision = 0.9835952807770876 , recall = 0.9788441692466461\n",
      "    f1 score = 0.9811265969802554 , AUC = 0.9788441692466461\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 14 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9900615435795077 , recall = 0.9864809081527348\n",
      "    f1 score = 0.9882186006286865 , AUC = 0.9864809081527348\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 15 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=10, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 1.0 , precision = 1.0 , recall = 1.0\n",
      "    f1 score = 1.0 , AUC = 1.0\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9550618108920814 , recall = 0.9503968253968254\n",
      "    f1 score = 0.9526381387619443 , AUC = 0.9503968253968254\n",
      "\n",
      "---- batch, 16 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9913793103448276 , recall = 0.9852941176470589\n",
      "    f1 score = 0.9881894873458792 , AUC = 0.9852941176470589\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 17 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9912087912087912 , precision = 0.9917973286875725 , recall = 0.989422084623323\n",
      "    f1 score = 0.9905863367401828 , AUC = 0.989422084623323\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9550618108920814 , recall = 0.9503968253968254\n",
      "    f1 score = 0.9526381387619443 , AUC = 0.9503968253968254\n",
      "\n",
      "---- batch, 18 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9912087912087912 , precision = 0.9917973286875725 , recall = 0.989422084623323\n",
      "    f1 score = 0.9905863367401828 , AUC = 0.989422084623323\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9736842105263157 , recall = 0.9523809523809523\n",
      "    f1 score = 0.9614864864864865 , AUC = 0.9523809523809523\n",
      "\n",
      "---- batch, 19 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9824175824175824 , precision = 0.9835952807770876 , recall = 0.9788441692466461\n",
      "    f1 score = 0.9811265969802554 , AUC = 0.9788441692466461\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9811507936507937 , recall = 0.9811507936507937\n",
      "    f1 score = 0.9811507936507937 , AUC = 0.9811507936507938\n",
      "\n",
      "---- batch, 20 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9956043956043956 , precision = 0.9953044375644995 , recall = 0.9953044375644995\n",
      "    f1 score = 0.9953044375644995 , AUC = 0.9953044375644995\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 21 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9900615435795077 , recall = 0.9864809081527348\n",
      "    f1 score = 0.9882186006286865 , AUC = 0.9864809081527348\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9811507936507937 , recall = 0.9811507936507937\n",
      "    f1 score = 0.9811507936507937 , AUC = 0.9811507936507938\n",
      "\n",
      "---- batch, 22 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9913793103448276 , recall = 0.9852941176470589\n",
      "    f1 score = 0.9881894873458792 , AUC = 0.9852941176470589\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9605128205128205 , recall = 0.9454365079365079\n",
      "    f1 score = 0.9521289997480473 , AUC = 0.945436507936508\n",
      "\n",
      "---- batch, 23 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9900615435795077 , recall = 0.9864809081527348\n",
      "    f1 score = 0.9882186006286865 , AUC = 0.9864809081527348\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9623015873015872 , recall = 0.9623015873015872\n",
      "    f1 score = 0.9623015873015872 , AUC = 0.9623015873015873\n",
      "\n",
      "---- batch, 24 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9900615435795077 , recall = 0.9864809081527348\n",
      "    f1 score = 0.9882186006286865 , AUC = 0.9864809081527348\n",
      "  - test set report -\n",
      "    accuracy = 0.9298245614035088 , precision = 0.9246031746031746 , recall = 0.9246031746031746\n",
      "    f1 score = 0.9246031746031746 , AUC = 0.9246031746031745\n",
      "\n",
      "---- batch, 25 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9846153846153847 , precision = 0.9853314204923487 , recall = 0.9817853457172343\n",
      "    f1 score = 0.9835060408801612 , AUC = 0.9817853457172343\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.9741062479117941 , recall = 0.9692460317460317\n",
      "    f1 score = 0.9715828832571667 , AUC = 0.9692460317460317\n",
      "\n",
      "---- batch, 26 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9802197802197802 , precision = 0.9806012974051896 , recall = 0.9770897832817338\n",
      "    f1 score = 0.9787934811316358 , AUC = 0.9770897832817338\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9623015873015872 , recall = 0.9623015873015872\n",
      "    f1 score = 0.9623015873015872 , AUC = 0.9623015873015873\n",
      "\n",
      "---- batch, 27 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=10, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9912087912087912 , precision = 0.9917973286875725 , recall = 0.989422084623323\n",
      "    f1 score = 0.9905863367401828 , AUC = 0.989422084623323\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9550618108920814 , recall = 0.9503968253968254\n",
      "    f1 score = 0.9526381387619443 , AUC = 0.9503968253968254\n",
      "\n",
      "---- batch, 28 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.9870789779326365 , recall = 0.9847265221878225\n",
      "    f1 score = 0.9858795051102742 , AUC = 0.9847265221878225\n",
      "  - test set report -\n",
      "    accuracy = 0.9385964912280702 , precision = 0.9360173738723689 , recall = 0.9315476190476191\n",
      "    f1 score = 0.933693394266722 , AUC = 0.9315476190476191\n",
      "\n",
      "---- batch, 29 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9846153846153847 , precision = 0.9853314204923487 , recall = 0.9817853457172343\n",
      "    f1 score = 0.9835060408801612 , AUC = 0.9817853457172343\n",
      "  - test set report -\n",
      "    accuracy = 0.9473684210526315 , precision = 0.9434523809523809 , recall = 0.9434523809523809\n",
      "    f1 score = 0.9434523809523809 , AUC = 0.943452380952381\n",
      "\n",
      "---- batch, 30 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1') \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.988337432776087 , recall = 0.9835397316821466\n",
      "    f1 score = 0.9858449477351916 , AUC = 0.9835397316821466\n",
      "  - test set report -\n",
      "    accuracy = 0.9912280701754386 , precision = 0.9931506849315068 , recall = 0.9880952380952381\n",
      "    f1 score = 0.9905276277523889 , AUC = 0.9880952380952381\n"
     ]
    }
   ],
   "source": [
    "#Supervised Learning: L1 Penalty SVM\n",
    "accuracy_train_arr, precision_train_arr, recall_train_arr, fscore_train_arr, auc_train_arr = [], [], [], [], [] \n",
    "accuracy_test_arr, precision_test_arr, recall_test_arr, fscore_test_arr, auc_test_arr = [], [], [], [], []\n",
    "\n",
    "for m in range(30):\n",
    "    \n",
    "    print(\"\\n---- batch,\", m+1, \"----\")\n",
    "    \n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=m, stratify=y)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train_normalized = scaler.fit_transform(X_train)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    \n",
    "    #parameter selection\n",
    "    parameters_l1 = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}\n",
    "    svm = LinearSVC(penalty='l1', dual=False, max_iter=100000)\n",
    "    clf = GridSearchCV(svm, parameters_l1, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    clf.fit(X_train_normalized, y_train)\n",
    "    best_svm = clf.best_estimator_\n",
    "    print(\"  (L1 Penalty SVM) Best estimator:\", clf.best_estimator_, '\\n')\n",
    "    \n",
    "    #extra processing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    preds_train = best_svm.predict(X_train_normalized)\n",
    "    preds_test = best_svm.predict(X_test_normalized)\n",
    "    \n",
    "    #report for train set\n",
    "    print(\"  - train set report -\")\n",
    "    accuracy_train = best_svm.score(X_train_normalized, y_train)\n",
    "    precision_train, recall_train, fscore_train, _ = precision_recall_fscore_support(y_train, preds_train, average='macro')\n",
    "    auc_train = roc_auc_score(lb.fit_transform(y_train),lb.transform(preds_train))\n",
    "    print(\"    accuracy =\", accuracy_train, \", precision =\", precision_train, \", recall =\", recall_train)\n",
    "    print(\"    f1 score =\", fscore_train, \", AUC =\", auc_train)\n",
    "    #print(metrics.classification_report(y_train, best_svm.predict(X_train_normalized)))\n",
    "    \n",
    "    #report for test set\n",
    "    print(\"  - test set report -\")\n",
    "    accuracy_test = best_svm.score(X_test_normalized, y_test)\n",
    "    precision_test, recall_test, fscore_test, _ = precision_recall_fscore_support(y_test, preds_test, average='macro')\n",
    "    auc_test = roc_auc_score(lb.fit_transform(y_test),lb.transform(preds_test))\n",
    "    print(\"    accuracy =\", accuracy_test, \", precision =\", precision_test, \", recall =\", recall_test)\n",
    "    print(\"    f1 score =\", fscore_test, \", AUC =\", auc_test)\n",
    "    #print(metrics.classification_report(y_test, best_svm.predict(X_test_normalized)))\n",
    "    \n",
    "    #metrics record\n",
    "    accuracy_train_arr.append(accuracy_train)\n",
    "    precision_train_arr.append(precision_train)\n",
    "    recall_train_arr.append(recall_train)\n",
    "    fscore_train_arr.append(fscore_train)\n",
    "    auc_train_arr.append(auc_train)\n",
    "    \n",
    "    accuracy_test_arr.append(accuracy_test)\n",
    "    precision_test_arr.append(precision_test)\n",
    "    recall_test_arr.append(recall_test)\n",
    "    fscore_test_arr.append(fscore_test)\n",
    "    auc_test_arr.append(auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8046e6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L1 Penalty SVM) Averages over 30 runs\n",
      "- train set -\n",
      "  average accuracy = 0.9875457875457876\n",
      "  average precision = 0.9885428899194267\n",
      "  average recall = 0.9848761609907124\n",
      "  average f1 score = 0.9866420274493166\n",
      "  average AUC = 0.9848761609907124\n",
      "- test set -\n",
      "  average accuracy = 0.9660818713450288\n",
      "  average precision = 0.9677794225913025\n",
      "  average recall = 0.959755291005291\n",
      "  average f1 score = 0.9632382321632648\n",
      "  average AUC = 0.9597552910052911\n"
     ]
    }
   ],
   "source": [
    "print(\"(L1 Penalty SVM) Averages over 30 runs\")\n",
    "print(\"- train set -\")\n",
    "print(\"  average accuracy =\", sum(accuracy_train_arr)/len(accuracy_train_arr))\n",
    "print(\"  average precision =\", sum(precision_train_arr)/len(precision_train_arr))\n",
    "print(\"  average recall =\", sum(recall_train_arr)/len(recall_train_arr))\n",
    "print(\"  average f1 score =\", sum(fscore_train_arr)/len(fscore_train_arr))\n",
    "print(\"  average AUC =\", sum(auc_train_arr)/len(auc_train_arr))\n",
    "print(\"- test set -\")\n",
    "print(\"  average accuracy =\", sum(accuracy_test_arr)/len(accuracy_test_arr))\n",
    "print(\"  average precision =\", sum(precision_test_arr)/len(precision_test_arr))\n",
    "print(\"  average recall =\", sum(recall_test_arr)/len(recall_test_arr))\n",
    "print(\"  average f1 score =\", sum(fscore_test_arr)/len(fscore_test_arr))\n",
    "print(\"  average AUC =\", sum(auc_test_arr)/len(auc_test_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ca298aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC curve & confusion matrices: random_state set to 29 \n",
      "\n",
      "- train set - (positive label = 'M')\n",
      "[[284   1]\n",
      " [  5 165]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test set - (positive label = 'M')\n",
      "[[72  0]\n",
      " [ 1 41]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('ROC curve & confusion matrices: random_state set to 29 \\n')\n",
    "print(\"- train set - (positive label = 'M')\")\n",
    "CM_train = metrics.confusion_matrix(y_train, preds_train)\n",
    "print(CM_train)\n",
    "RocCurveDisplay.from_predictions(lb.fit_transform(y_train),lb.transform(preds_train))\n",
    "plt.show()\n",
    "\n",
    "print(\"- test set - (positive label = 'M')\")\n",
    "CM_test = metrics.confusion_matrix(y_test, preds_test)\n",
    "print(CM_test)\n",
    "RocCurveDisplay.from_predictions(lb.fit_transform(y_test),lb.transform(preds_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac82a6",
   "metadata": {},
   "source": [
    "#### 1(b)ii. Semi-Supervised Learning/ Self-training: select 50% of the positive class along with 50% of the negative class in the training set as labeled data and the rest as unlabelled data. You can select them randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc0b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- batch, 1 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9736842105263158 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9846153846153847 , precision = 0.98662486938349 , recall = 0.9805985552115584\n",
      "    f1 score = 0.9834652822842311 , AUC = 0.9805985552115584\n",
      "  - test set report -\n",
      "    accuracy = 0.9385964912280702 , precision = 0.9323616115296429 , recall = 0.9365079365079365\n",
      "    f1 score = 0.9343480049362403 , AUC = 0.9365079365079365\n",
      "\n",
      "---- batch, 2 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.956140350877193 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9758241758241758 , precision = 0.9784540717707371 , recall = 0.9700206398348813\n",
      "    f1 score = 0.9739514210918023 , AUC = 0.9700206398348813\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9550618108920814 , recall = 0.9503968253968254\n",
      "    f1 score = 0.9526381387619443 , AUC = 0.9503968253968254\n",
      "\n",
      "---- batch, 3 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9736842105263158 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9714285714285714 , precision = 0.975081336882579 , recall = 0.9641382868937048\n",
      "    f1 score = 0.9691362856054557 , AUC = 0.9641382868937048\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.9741062479117941 , recall = 0.9692460317460317\n",
      "    f1 score = 0.9715828832571667 , AUC = 0.9692460317460317\n",
      "\n",
      "---- batch, 4 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9736842105263158 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9714285714285714 , precision = 0.9711410512308716 , recall = 0.9676986584107327\n",
      "    f1 score = 0.969368361634585 , AUC = 0.9676986584107327\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9510317720275139 , recall = 0.9553571428571428\n",
      "    f1 score = 0.9531057178116 , AUC = 0.9553571428571429\n",
      "\n",
      "---- batch, 5 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9473684210526315 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9846153846153847 , precision = 0.9830224034264063 , recall = 0.9841589267285862\n",
      "    f1 score = 0.9835849280262229 , AUC = 0.9841589267285862\n",
      "  - test set report -\n",
      "    accuracy = 0.9298245614035088 , precision = 0.9287162162162163 , recall = 0.9196428571428572\n",
      "    f1 score = 0.9238222519211494 , AUC = 0.9196428571428572\n",
      "\n",
      "---- batch, 6 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9649122807017544 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9758241758241758 , precision = 0.9784540717707371 , recall = 0.9700206398348813\n",
      "    f1 score = 0.9739514210918023 , AUC = 0.9700206398348813\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9736842105263157 , recall = 0.9523809523809523\n",
      "    f1 score = 0.9614864864864865 , AUC = 0.9523809523809523\n",
      "\n",
      "---- batch, 7 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=100000, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9649122807017544 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9900615435795077 , recall = 0.9864809081527348\n",
      "    f1 score = 0.9882186006286865 , AUC = 0.9864809081527348\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 8 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=10, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9473684210526315 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9802197802197802 , precision = 0.9773398106014021 , recall = 0.9806501547987616\n",
      "    f1 score = 0.9789439585357953 , AUC = 0.9806501547987617\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9736842105263157 , recall = 0.9523809523809523\n",
      "    f1 score = 0.9614864864864865 , AUC = 0.9523809523809523\n",
      "\n",
      "---- batch, 9 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9824561403508771 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.978021978021978 , precision = 0.9815552184721695 , recall = 0.9717750257997936\n",
      "    f1 score = 0.9762892400050027 , AUC = 0.9717750257997936\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9864864864864865 , recall = 0.9761904761904762\n",
      "    f1 score = 0.9809555629802873 , AUC = 0.9761904761904762\n",
      "\n",
      "---- batch, 10 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9473684210526315 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9758241758241758 , precision = 0.9784540717707371 , recall = 0.9700206398348813\n",
      "    f1 score = 0.9739514210918023 , AUC = 0.9700206398348813\n",
      "  - test set report -\n",
      "    accuracy = 0.9298245614035088 , precision = 0.9412393162393162 , recall = 0.9097222222222223\n",
      "    f1 score = 0.9220512820512821 , AUC = 0.9097222222222224\n",
      "\n",
      "---- batch, 11 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9473684210526315 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9736263736263736 , precision = 0.9767623140774448 , recall = 0.9670794633642931\n",
      "    f1 score = 0.9715470880060033 , AUC = 0.9670794633642931\n",
      "  - test set report -\n",
      "    accuracy = 0.9473684210526315 , precision = 0.9539473684210527 , recall = 0.933531746031746\n",
      "    f1 score = 0.9422297297297297 , AUC = 0.933531746031746\n",
      "\n",
      "---- batch, 12 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.956140350877193 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.978021978021978 , precision = 0.9801567345570363 , recall = 0.9729618163054695\n",
      "    f1 score = 0.9763493845642048 , AUC = 0.9729618163054695\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9736842105263157 , recall = 0.9523809523809523\n",
      "    f1 score = 0.9614864864864865 , AUC = 0.9523809523809523\n",
      "\n",
      "---- batch, 13 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9649122807017544 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9802197802197802 , precision = 0.9818704284221526 , recall = 0.9759029927760579\n",
      "    f1 score = 0.9787410772225827 , AUC = 0.9759029927760579\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 14 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.956140350877193 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9736263736263736 , precision = 0.9729239256678281 , recall = 0.970639834881321\n",
      "    f1 score = 0.9717590102205487 , AUC = 0.970639834881321\n",
      "  - test set report -\n",
      "    accuracy = 0.956140350877193 , precision = 0.9510317720275139 , recall = 0.9553571428571428\n",
      "    f1 score = 0.9531057178116 , AUC = 0.9553571428571429\n",
      "\n",
      "---- batch, 15 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.956140350877193 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.988337432776087 , recall = 0.9835397316821466\n",
      "    f1 score = 0.9858449477351916 , AUC = 0.9835397316821466\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9623015873015872 , recall = 0.9623015873015872\n",
      "    f1 score = 0.9623015873015872 , AUC = 0.9623015873015873\n",
      "\n",
      "---- batch, 16 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9912280701754386 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.978021978021978 , precision = 0.9776422764227641 , recall = 0.9753353973168215\n",
      "    f1 score = 0.9764658418504573 , AUC = 0.9753353973168215\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9864864864864865 , recall = 0.9761904761904762\n",
      "    f1 score = 0.9809555629802873 , AUC = 0.9761904761904762\n",
      "\n",
      "---- batch, 17 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9473684210526315 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - train set report -\n",
      "    accuracy = 0.9802197802197802 , precision = 0.9773398106014021 , recall = 0.9806501547987616\n",
      "    f1 score = 0.9789439585357953 , AUC = 0.9806501547987617\n",
      "  - test set report -\n",
      "    accuracy = 0.9385964912280702 , precision = 0.929951690821256 , recall = 0.941468253968254\n",
      "    f1 score = 0.9349474199070678 , AUC = 0.941468253968254\n",
      "\n",
      "---- batch, 18 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1000, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9736842105263158 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9868131868131869 , precision = 0.988337432776087 , recall = 0.9835397316821466\n",
      "    f1 score = 0.9858449477351916 , AUC = 0.9835397316821466\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9672297297297296 , recall = 0.9573412698412699\n",
      "    f1 score = 0.9619111259605747 , AUC = 0.9573412698412699\n",
      "\n",
      "---- batch, 19 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9912280701754386 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9758241758241758 , precision = 0.9758711743180306 , recall = 0.9723942208462333\n",
      "    f1 score = 0.9740809213831104 , AUC = 0.9723942208462333\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9811507936507937 , recall = 0.9811507936507937\n",
      "    f1 score = 0.9811507936507937 , AUC = 0.9811507936507938\n",
      "\n",
      "---- batch, 20 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=10, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9736842105263158 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9846153846153847 , precision = 0.9830224034264063 , recall = 0.9841589267285862\n",
      "    f1 score = 0.9835849280262229 , AUC = 0.9841589267285862\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.98 , recall = 0.9642857142857143\n",
      "    f1 score = 0.9712773998488284 , AUC = 0.9642857142857143\n",
      "\n",
      "---- batch, 21 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9649122807017544 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9802197802197802 , precision = 0.9783378634379376 , recall = 0.9794633642930857\n",
      "    f1 score = 0.9788949074622866 , AUC = 0.9794633642930857\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.9697019325253848 , recall = 0.9742063492063492\n",
      "    f1 score = 0.97186343068696 , AUC = 0.9742063492063492\n",
      "\n",
      "---- batch, 22 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9649122807017544 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9758241758241758 , precision = 0.9814189189189189 , recall = 0.9676470588235294\n",
      "    f1 score = 0.9738162375947559 , AUC = 0.9676470588235294\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9864864864864865 , recall = 0.9761904761904762\n",
      "    f1 score = 0.9809555629802873 , AUC = 0.9761904761904762\n",
      "\n",
      "---- batch, 23 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.956140350877193 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.978021978021978 , precision = 0.9754910017256965 , recall = 0.9777089783281734\n",
      "    f1 score = 0.9765773000576559 , AUC = 0.9777089783281735\n",
      "  - test set report -\n",
      "    accuracy = 0.9736842105263158 , precision = 0.9697019325253848 , recall = 0.9742063492063492\n",
      "    f1 score = 0.97186343068696 , AUC = 0.9742063492063492\n",
      "\n",
      "---- batch, 24 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=10000, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9649122807017544 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9714285714285714 , precision = 0.9689687834609999 , recall = 0.9700722394220846\n",
      "    f1 score = 0.969514866334414 , AUC = 0.9700722394220846\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9623015873015872 , recall = 0.9623015873015872\n",
      "    f1 score = 0.9623015873015872 , AUC = 0.9623015873015873\n",
      "\n",
      "---- batch, 25 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9649122807017544 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9758241758241758 , precision = 0.9784540717707371 , recall = 0.9700206398348813\n",
      "    f1 score = 0.9739514210918023 , AUC = 0.9700206398348813\n",
      "  - test set report -\n",
      "    accuracy = 0.9824561403508771 , precision = 0.9864864864864865 , recall = 0.9761904761904762\n",
      "    f1 score = 0.9809555629802873 , AUC = 0.9761904761904762\n",
      "\n",
      "---- batch, 26 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.956140350877193 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9736263736263736 , precision = 0.9729239256678281 , recall = 0.970639834881321\n",
      "    f1 score = 0.9717590102205487 , AUC = 0.970639834881321\n",
      "  - test set report -\n",
      "    accuracy = 0.9473684210526315 , precision = 0.9434523809523809 , recall = 0.9434523809523809\n",
      "    f1 score = 0.9434523809523809 , AUC = 0.943452380952381\n",
      "\n",
      "---- batch, 27 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9649122807017544 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9692307692307692 , precision = 0.9765886287625418 , recall = 0.9588235294117646\n",
      "    f1 score = 0.9665413059921002 , AUC = 0.9588235294117646\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9623015873015872 , recall = 0.9623015873015872\n",
      "    f1 score = 0.9623015873015872 , AUC = 0.9623015873015873\n",
      "\n",
      "---- batch, 28 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=0.1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9035087719298246 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9714285714285714 , precision = 0.970010758472297 , recall = 0.9688854489164087\n",
      "    f1 score = 0.969442421048825 , AUC = 0.9688854489164087\n",
      "  - test set report -\n",
      "    accuracy = 0.9210526315789473 , precision = 0.9169729368526562 , recall = 0.9126984126984127\n",
      "    f1 score = 0.9147486497714998 , AUC = 0.9126984126984126\n",
      "\n",
      "---- batch, 29 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9736842105263158 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.989010989010989 , precision = 0.9913793103448276 , recall = 0.9852941176470589\n",
      "    f1 score = 0.9881894873458792 , AUC = 0.9852941176470589\n",
      "  - test set report -\n",
      "    accuracy = 0.9649122807017544 , precision = 0.9672297297297296 , recall = 0.9573412698412699\n",
      "    f1 score = 0.9619111259605747 , AUC = 0.9573412698412699\n",
      "\n",
      "---- batch, 30 ----\n",
      "  (L1 Penalty SVM) Best estimator: LinearSVC(C=1, dual=False, max_iter=100000, penalty='l1')\n",
      "   --score with only labeled samples: 0.9824561403508771 \n",
      "\n",
      "  - train set report -\n",
      "    accuracy = 0.9604395604395605 , precision = 0.9551564027370478 , recall = 0.9613003095975232\n",
      "    f1 score = 0.9580309924978478 , AUC = 0.9613003095975232\n",
      "  - test set report -\n",
      "    accuracy = 0.9912280701754386 , precision = 0.9883720930232558 , recall = 0.9930555555555556\n",
      "    f1 score = 0.99062114356232 , AUC = 0.9930555555555556\n"
     ]
    }
   ],
   "source": [
    "# Semi-Supervised Learning: L1 penalty SVM\n",
    "accuracy_train_arr, precision_train_arr, recall_train_arr, fscore_train_arr, auc_train_arr = [], [], [], [], [] \n",
    "accuracy_test_arr, precision_test_arr, recall_test_arr, fscore_test_arr, auc_test_arr = [], [], [], [], []\n",
    "\n",
    "for m in range(30):\n",
    "    \n",
    "    print(\"\\n---- batch,\", m+1, \"----\")\n",
    "    \n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=m, stratify=y)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train_normalized = scaler.fit_transform(X_train)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    \n",
    "    #label unlabeled split\n",
    "    X_labeled, X_unlabeled, y_labeled, y_unlabeled = train_test_split(\n",
    "        X_train_normalized, y_train, test_size=0.5, random_state=m, stratify=y_train)\n",
    "    \n",
    "    #parameter selection\n",
    "    parameters_l1 = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}\n",
    "    svm = LinearSVC(penalty='l1', dual=False, max_iter=100000)\n",
    "    clf = GridSearchCV(svm, parameters_l1, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "    clf.fit(X_labeled, y_labeled)\n",
    "    best_svm = clf.best_estimator_\n",
    "    best_svm.fit(X_labeled, y_labeled)\n",
    "    print(\"  (L1 Penalty SVM) Best estimator:\", clf.best_estimator_)\n",
    "    print(\"   --score with only labeled samples:\", best_svm.score(X_test_normalized, y_test), '\\n')\n",
    "    \n",
    "    iter_num = y_unlabeled.shape[0]\n",
    "    for i in range(iter_num):\n",
    "        confidence_scores = best_svm.decision_function(X_unlabeled)\n",
    "        distance = np.abs(confidence_scores)\n",
    "        idx_max = distance.argmax()\n",
    "    \n",
    "        #add to labeled set\n",
    "        X_labeled = np.append(X_labeled,[X_unlabeled[idx_max]], axis=0)\n",
    "        #M - distance > 0 vs. B - distance < 0\n",
    "        if(confidence_scores[idx_max] < 0):\n",
    "            y_labeled = y_labeled.append(pd.Series('B'))\n",
    "        else:\n",
    "            y_labeled = y_labeled.append(pd.Series('M'))\n",
    "    \n",
    "        #delete from unlabeled set\n",
    "        X_unlabeled = np.delete(X_unlabeled, idx_max ,0)\n",
    "\n",
    "        #re-train\n",
    "        best_svm.fit(X_labeled, y_labeled)\n",
    "    \n",
    "    #extra processing\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    preds_train = best_svm.predict(X_train_normalized)\n",
    "    preds_test = best_svm.predict(X_test_normalized)\n",
    "    \n",
    "    #report for train set\n",
    "    print(\"  - train set report -\")\n",
    "    accuracy_train = best_svm.score(X_train_normalized, y_train)\n",
    "    precision_train, recall_train, fscore_train, _ = precision_recall_fscore_support(y_train, preds_train, average='macro')\n",
    "    auc_train = roc_auc_score(lb.fit_transform(y_train),lb.transform(preds_train))\n",
    "    print(\"    accuracy =\", accuracy_train, \", precision =\", precision_train, \", recall =\", recall_train)\n",
    "    print(\"    f1 score =\", fscore_train, \", AUC =\", auc_train)\n",
    "    \n",
    "    #report for test set\n",
    "    print(\"  - test set report -\")\n",
    "    accuracy_test = best_svm.score(X_test_normalized, y_test)\n",
    "    precision_test, recall_test, fscore_test, _ = precision_recall_fscore_support(y_test, preds_test, average='macro')\n",
    "    auc_test = roc_auc_score(lb.fit_transform(y_test),lb.transform(preds_test))\n",
    "    print(\"    accuracy =\", accuracy_test, \", precision =\", precision_test, \", recall =\", recall_test)\n",
    "    print(\"    f1 score =\", fscore_test, \", AUC =\", auc_test)\n",
    "    \n",
    "    #metrics record\n",
    "    accuracy_train_arr.append(accuracy_train)\n",
    "    precision_train_arr.append(precision_train)\n",
    "    recall_train_arr.append(recall_train)\n",
    "    fscore_train_arr.append(fscore_train)\n",
    "    auc_train_arr.append(auc_train)\n",
    "    \n",
    "    accuracy_test_arr.append(accuracy_test)\n",
    "    precision_test_arr.append(precision_test)\n",
    "    recall_test_arr.append(recall_test)\n",
    "    fscore_test_arr.append(fscore_test)\n",
    "    auc_test_arr.append(auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96b29b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-Supervised (L1 Penalty SVM) Averages over 30 runs\n",
      "- train set -\n",
      "  average accuracy = 0.9776556776556777\n",
      "  average precision = 0.978372734907757\n",
      "  average recall = 0.9740540075679395\n",
      "  average f1 score = 0.9760246991640268\n",
      "  average AUC = 0.9740540075679396\n",
      "- test set -\n",
      "  average accuracy = 0.9628654970760232\n",
      "  average precision = 0.9638387558168456\n",
      "  average recall = 0.9568783068783068\n",
      "  average f1 score = 0.9598123966750568\n",
      "  average AUC = 0.9568783068783068\n"
     ]
    }
   ],
   "source": [
    "print(\"Semi-Supervised (L1 Penalty SVM) Averages over 30 runs\")\n",
    "print(\"- train set -\")\n",
    "print(\"  average accuracy =\", sum(accuracy_train_arr)/len(accuracy_train_arr))\n",
    "print(\"  average precision =\", sum(precision_train_arr)/len(precision_train_arr))\n",
    "print(\"  average recall =\", sum(recall_train_arr)/len(recall_train_arr))\n",
    "print(\"  average f1 score =\", sum(fscore_train_arr)/len(fscore_train_arr))\n",
    "print(\"  average AUC =\", sum(auc_train_arr)/len(auc_train_arr))\n",
    "print(\"- test set -\")\n",
    "print(\"  average accuracy =\", sum(accuracy_test_arr)/len(accuracy_test_arr))\n",
    "print(\"  average precision =\", sum(precision_test_arr)/len(precision_test_arr))\n",
    "print(\"  average recall =\", sum(recall_test_arr)/len(recall_test_arr))\n",
    "print(\"  average f1 score =\", sum(fscore_test_arr)/len(fscore_test_arr))\n",
    "print(\"  average AUC =\", sum(auc_test_arr)/len(auc_test_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c351f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC curve & confusion matrices: random_state set to 29 \n",
      "\n",
      "- train set - (positive label = 'M')\n",
      "[[273  12]\n",
      " [  6 164]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test set - (positive label = 'M')\n",
      "[[71  1]\n",
      " [ 0 42]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('ROC curve & confusion matrices: random_state set to 29 \\n')\n",
    "print(\"- train set - (positive label = 'M')\")\n",
    "CM_train = metrics.confusion_matrix(y_train, preds_train)\n",
    "print(CM_train)\n",
    "RocCurveDisplay.from_predictions(lb.fit_transform(y_train),lb.transform(preds_train))\n",
    "plt.show()\n",
    "\n",
    "print(\"- test set - (positive label = 'M')\")\n",
    "CM_test = metrics.confusion_matrix(y_test, preds_test)\n",
    "print(CM_test)\n",
    "RocCurveDisplay.from_predictions(lb.fit_transform(y_test),lb.transform(preds_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462be9ba",
   "metadata": {},
   "source": [
    "#### 1(b)iii. Unsupervised Learning: Run k-means algorithm on the whole training set. Ignore the labels of the data, and assume k = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b227df4",
   "metadata": {},
   "source": [
    "#### A. Run the k-means algorithm multiple times. Make sure that you initialize the algoritm randomly. How do you make sure that the algorithm was not trapped in a local minimum?\n",
    "\n",
    "- control the max_iter\n",
    "- use stratify to include both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b72c9e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- batch, 1 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8461538461538461 , precision = 0.8973643281675276 , recall = 0.7953044375644995\n",
      "    f1 score = 0.8164645952424857 , AUC = 0.7953044375644995\n",
      "  - test set report -\n",
      "    accuracy = 0.8859649122807017 , precision = 0.9235294117647059 , recall = 0.8452380952380952\n",
      "    f1 score = 0.8670494303400018 , AUC = 0.8452380952380952\n",
      "\n",
      "---- batch, 2 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8263736263736263 , precision = 0.8868888492841442 , recall = 0.7688338493292053\n",
      "    f1 score = 0.7887588812947737 , AUC = 0.7688338493292053\n",
      "  - test set report -\n",
      "    accuracy = 0.8771929824561403 , precision = 0.9186046511627908 , recall = 0.8333333333333333\n",
      "    f1 score = 0.8556962025316456 , AUC = 0.8333333333333333\n",
      "\n",
      "---- batch, 3 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8263736263736263 , precision = 0.8914835164835164 , recall = 0.7676470588235293\n",
      "    f1 score = 0.7877961378838059 , AUC = 0.7676470588235293\n",
      "  - test set report -\n",
      "    accuracy = 0.868421052631579 , precision = 0.9004056795131846 , recall = 0.8263888888888888\n",
      "    f1 score = 0.846595496546156 , AUC = 0.8263888888888888\n",
      "\n",
      "---- batch, 4 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8461538461538461 , precision = 0.8973643281675276 , recall = 0.7953044375644995\n",
      "    f1 score = 0.8164645952424857 , AUC = 0.7953044375644995\n",
      "  - test set report -\n",
      "    accuracy = 0.8859649122807017 , precision = 0.9235294117647059 , recall = 0.8452380952380952\n",
      "    f1 score = 0.8670494303400018 , AUC = 0.8452380952380952\n",
      "\n",
      "---- batch, 5 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8571428571428571 , precision = 0.9033730798152326 , recall = 0.8100103199174407\n",
      "    f1 score = 0.8313286681380854 , AUC = 0.8100103199174407\n",
      "  - test set report -\n",
      "    accuracy = 0.8421052631578947 , precision = 0.9 , recall = 0.7857142857142857\n",
      "    f1 score = 0.8080808080808082 , AUC = 0.7857142857142857\n",
      "\n",
      "---- batch, 6 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8549450549450549 , precision = 0.9021598096988701 , recall = 0.8070691434468524\n",
      "    f1 score = 0.8283843094225758 , AUC = 0.8070691434468524\n",
      "  - test set report -\n",
      "    accuracy = 0.8508771929824561 , precision = 0.9044943820224719 , recall = 0.7976190476190477\n",
      "    f1 score = 0.8203392973023083 , AUC = 0.7976190476190477\n",
      "\n",
      "---- batch, 7 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8505494505494505 , precision = 0.8997507122507122 , recall = 0.801186790505676\n",
      "    f1 score = 0.8224532892622687 , AUC = 0.801186790505676\n",
      "  - test set report -\n",
      "    accuracy = 0.868421052631579 , precision = 0.9137931034482758 , recall = 0.8214285714285714\n",
      "    f1 score = 0.8441345365053323 , AUC = 0.8214285714285714\n",
      "\n",
      "---- batch, 8 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8417582417582418 , precision = 0.895 , recall = 0.7894220846233231\n",
      "    f1 score = 0.8104166666666668 , AUC = 0.7894220846233231\n",
      "  - test set report -\n",
      "    accuracy = 0.8508771929824561 , precision = 0.9044943820224719 , recall = 0.7976190476190477\n",
      "    f1 score = 0.8203392973023083 , AUC = 0.7976190476190477\n",
      "\n",
      "---- batch, 9 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8549450549450549 , precision = 0.9021598096988701 , recall = 0.8070691434468524\n",
      "    f1 score = 0.8283843094225758 , AUC = 0.8070691434468524\n",
      "  - test set report -\n",
      "    accuracy = 0.8508771929824561 , precision = 0.9044943820224719 , recall = 0.7976190476190477\n",
      "    f1 score = 0.8203392973023083 , AUC = 0.7976190476190477\n",
      "\n",
      "---- batch, 10 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8549450549450549 , precision = 0.9021598096988701 , recall = 0.8070691434468524\n",
      "    f1 score = 0.8283843094225758 , AUC = 0.8070691434468524\n",
      "  - test set report -\n",
      "    accuracy = 0.8508771929824561 , precision = 0.9044943820224719 , recall = 0.7976190476190477\n",
      "    f1 score = 0.8203392973023083 , AUC = 0.7976190476190477\n",
      "\n",
      "---- batch, 11 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8505494505494505 , precision = 0.8997507122507122 , recall = 0.801186790505676\n",
      "    f1 score = 0.8224532892622687 , AUC = 0.801186790505676\n",
      "  - test set report -\n",
      "    accuracy = 0.8157894736842105 , precision = 0.8870967741935484 , recall = 0.75\n",
      "    f1 score = 0.7696969696969697 , AUC = 0.75\n",
      "\n",
      "---- batch, 12 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8527472527472527 , precision = 0.900952380952381 , recall = 0.8041279669762642\n",
      "    f1 score = 0.8254259126700072 , AUC = 0.8041279669762642\n",
      "  - test set report -\n",
      "    accuracy = 0.8596491228070176 , precision = 0.9090909090909092 , recall = 0.8095238095238095\n",
      "    f1 score = 0.8323529411764706 , AUC = 0.8095238095238095\n",
      "\n",
      "---- batch, 13 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8637362637362638 , precision = 0.9070487483530962 , recall = 0.8188338493292053\n",
      "    f1 score = 0.8400793650793652 , AUC = 0.8188338493292053\n",
      "  - test set report -\n",
      "    accuracy = 0.8157894736842105 , precision = 0.8870967741935484 , recall = 0.75\n",
      "    f1 score = 0.7696969696969697 , AUC = 0.75\n",
      "\n",
      "---- batch, 14 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8549450549450549 , precision = 0.9021598096988701 , recall = 0.8070691434468524\n",
      "    f1 score = 0.8283843094225758 , AUC = 0.8070691434468524\n",
      "  - test set report -\n",
      "    accuracy = 0.8508771929824561 , precision = 0.9044943820224719 , recall = 0.7976190476190477\n",
      "    f1 score = 0.8203392973023083 , AUC = 0.7976190476190477\n",
      "\n",
      "---- batch, 15 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8593406593406593 , precision = 0.9083094555873925 , recall = 0.8117647058823529\n",
      "    f1 score = 0.833584784894619 , AUC = 0.8117647058823529\n",
      "  - test set report -\n",
      "    accuracy = 0.8333333333333334 , precision = 0.878876404494382 , recall = 0.7787698412698413\n",
      "    f1 score = 0.7992027440437564 , AUC = 0.7787698412698413\n",
      "\n",
      "---- batch, 16 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8527472527472527 , precision = 0.900952380952381 , recall = 0.8041279669762642\n",
      "    f1 score = 0.8254259126700072 , AUC = 0.8041279669762642\n",
      "  - test set report -\n",
      "    accuracy = 0.8596491228070176 , precision = 0.9090909090909092 , recall = 0.8095238095238095\n",
      "    f1 score = 0.8323529411764706 , AUC = 0.8095238095238095\n",
      "\n",
      "---- batch, 17 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8571428571428571 , precision = 0.9033730798152326 , recall = 0.8100103199174407\n",
      "    f1 score = 0.8313286681380854 , AUC = 0.8100103199174407\n",
      "  - test set report -\n",
      "    accuracy = 0.8421052631578947 , precision = 0.9 , recall = 0.7857142857142857\n",
      "    f1 score = 0.8080808080808082 , AUC = 0.7857142857142857\n",
      "\n",
      "---- batch, 18 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8593406593406593 , precision = 0.9045922723876614 , recall = 0.8129514963880289\n",
      "    f1 score = 0.8342591749385302 , AUC = 0.8129514963880289\n",
      "  - test set report -\n",
      "    accuracy = 0.8333333333333334 , precision = 0.8956043956043955 , recall = 0.7738095238095238\n",
      "    f1 score = 0.7955639452571968 , AUC = 0.7738095238095238\n",
      "\n",
      "---- batch, 19 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8505494505494505 , precision = 0.8997507122507122 , recall = 0.801186790505676\n",
      "    f1 score = 0.8224532892622687 , AUC = 0.801186790505676\n",
      "  - test set report -\n",
      "    accuracy = 0.868421052631579 , precision = 0.9137931034482758 , recall = 0.8214285714285714\n",
      "    f1 score = 0.8441345365053323 , AUC = 0.8214285714285714\n",
      "\n",
      "---- batch, 20 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8373626373626374 , precision = 0.89265706282513 , recall = 0.7835397316821465\n",
      "    f1 score = 0.8043078997535686 , AUC = 0.7835397316821465\n",
      "  - test set report -\n",
      "    accuracy = 0.8508771929824561 , precision = 0.9044943820224719 , recall = 0.7976190476190477\n",
      "    f1 score = 0.8203392973023083 , AUC = 0.7976190476190477\n",
      "\n",
      "---- batch, 21 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8395604395604396 , precision = 0.893825899443877 , recall = 0.7864809081527347\n",
      "    f1 score = 0.8073699899668849 , AUC = 0.7864809081527347\n",
      "  - test set report -\n",
      "    accuracy = 0.8421052631578947 , precision = 0.9 , recall = 0.7857142857142857\n",
      "    f1 score = 0.8080808080808082 , AUC = 0.7857142857142857\n",
      "\n",
      "---- batch, 22 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8571428571428571 , precision = 0.9033730798152326 , recall = 0.8100103199174407\n",
      "    f1 score = 0.8313286681380854 , AUC = 0.8100103199174407\n",
      "  - test set report -\n",
      "    accuracy = 0.8333333333333334 , precision = 0.8956043956043955 , recall = 0.7738095238095238\n",
      "    f1 score = 0.7955639452571968 , AUC = 0.7738095238095238\n",
      "\n",
      "---- batch, 23 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - train set report -\n",
      "    accuracy = 0.8483516483516483 , precision = 0.898554721977052 , recall = 0.7982456140350878\n",
      "    f1 score = 0.8194662480376766 , AUC = 0.7982456140350878\n",
      "  - test set report -\n",
      "    accuracy = 0.8245614035087719 , precision = 0.8913043478260869 , recall = 0.7619047619047619\n",
      "    f1 score = 0.7827743902439026 , AUC = 0.7619047619047619\n",
      "\n",
      "---- batch, 24 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8571428571428571 , precision = 0.9071428571428571 , recall = 0.8088235294117647\n",
      "    f1 score = 0.8306370794559771 , AUC = 0.8088235294117647\n",
      "  - test set report -\n",
      "    accuracy = 0.8421052631578947 , precision = 0.8841783216783217 , recall = 0.7906746031746033\n",
      "    f1 score = 0.8113970588235294 , AUC = 0.7906746031746033\n",
      "\n",
      "---- batch, 25 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8461538461538461 , precision = 0.8973643281675276 , recall = 0.7953044375644995\n",
      "    f1 score = 0.8164645952424857 , AUC = 0.7953044375644995\n",
      "  - test set report -\n",
      "    accuracy = 0.8859649122807017 , precision = 0.9235294117647059 , recall = 0.8452380952380952\n",
      "    f1 score = 0.8670494303400018 , AUC = 0.8452380952380952\n",
      "\n",
      "---- batch, 26 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8351648351648352 , precision = 0.8914934055174797 , recall = 0.7805985552115583\n",
      "    f1 score = 0.801230188547364 , AUC = 0.7805985552115583\n",
      "  - test set report -\n",
      "    accuracy = 0.868421052631579 , precision = 0.9137931034482758 , recall = 0.8214285714285714\n",
      "    f1 score = 0.8441345365053323 , AUC = 0.8214285714285714\n",
      "\n",
      "---- batch, 27 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8461538461538461 , precision = 0.8973643281675276 , recall = 0.7953044375644995\n",
      "    f1 score = 0.8164645952424857 , AUC = 0.7953044375644995\n",
      "  - test set report -\n",
      "    accuracy = 0.8771929824561403 , precision = 0.9186046511627908 , recall = 0.8333333333333333\n",
      "    f1 score = 0.8556962025316456 , AUC = 0.8333333333333333\n",
      "\n",
      "---- batch, 28 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8549450549450549 , precision = 0.9021598096988701 , recall = 0.8070691434468524\n",
      "    f1 score = 0.8283843094225758 , AUC = 0.8070691434468524\n",
      "  - test set report -\n",
      "    accuracy = 0.8508771929824561 , precision = 0.9044943820224719 , recall = 0.7976190476190477\n",
      "    f1 score = 0.8203392973023083 , AUC = 0.7976190476190477\n",
      "\n",
      "---- batch, 29 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8637362637362638 , precision = 0.9070487483530962 , recall = 0.8188338493292053\n",
      "    f1 score = 0.8400793650793652 , AUC = 0.8188338493292053\n",
      "  - test set report -\n",
      "    accuracy = 0.8421052631578947 , precision = 0.9 , recall = 0.7857142857142857\n",
      "    f1 score = 0.8080808080808082 , AUC = 0.7857142857142857\n",
      "\n",
      "---- batch, 30 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.8417582417582418 , precision = 0.895 , recall = 0.7894220846233231\n",
      "    f1 score = 0.8104166666666668 , AUC = 0.7894220846233231\n",
      "  - test set report -\n",
      "    accuracy = 0.8947368421052632 , precision = 0.9285714285714286 , recall = 0.8571428571428572\n",
      "    f1 score = 0.8782051282051282 , AUC = 0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "# Unsupervised Learning: kmeans\n",
    "accuracy_train_arr, precision_train_arr, recall_train_arr, fscore_train_arr, auc_train_arr = [], [], [], [], [] \n",
    "accuracy_test_arr, precision_test_arr, recall_test_arr, fscore_test_arr, auc_test_arr = [], [], [], [], []\n",
    "\n",
    "for m in range(30):\n",
    "    print(\"\\n---- batch,\", m+1, \"----\")\n",
    "    \n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=m, stratify=y)\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "    \n",
    "    #kmeans\n",
    "    kmeans = KMeans(n_clusters=2, init='random', max_iter=10000, random_state=m).fit(X_train)\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    cluster0_idx_arr, cluster1_idx_arr = np.where(labels==0), np.where(labels==1)\n",
    "    distances_0, distances_1 = np.array([]), np.array([])\n",
    "    true_label_0, true_label_1 = np.array([]), np.array([])\n",
    "    \n",
    "    #decide cluster label\n",
    "    for i in cluster0_idx_arr[0]:\n",
    "        dist = np.linalg.norm(X_train[i] - centers[0])\n",
    "        distances_0 = np.append(distances_0, dist)\n",
    "        true_label_0 = np.append(true_label_0, y_train[i])  \n",
    "    closest30_0 = true_label_0[np.argsort(distances_0)[:30]]\n",
    "    \n",
    "    for i in cluster1_idx_arr[0]:\n",
    "        dist = np.linalg.norm(X_train[i] - centers[1])\n",
    "        distances_1 = np.append(distances_1, dist)\n",
    "        true_label_1 = np.append(true_label_1, y_train[i])\n",
    "    closest30_1 = true_label_1[np.argsort(distances_1)[:30]]\n",
    "    \n",
    "    cluster0_m_cnt = 0\n",
    "    for i in range(30):\n",
    "        if closest30_0[i]=='M':\n",
    "            cluster0_m_cnt = cluster0_m_cnt + 1\n",
    "    if cluster0_m_cnt > 15: \n",
    "        cluster_label_0 = 'M'\n",
    "        cluster_label_1 = 'B'\n",
    "    else:\n",
    "        cluster_label_0 = 'B'\n",
    "        cluster_label_1 = 'M'\n",
    "    \n",
    "    #make preds arr according to cluster labels\n",
    "    preds_train, preds_test = [], []\n",
    "    for i in range(labels.shape[0]):\n",
    "        if labels[i] == 0:\n",
    "            preds_train.append(cluster_label_0)\n",
    "        else:\n",
    "            preds_train.append(cluster_label_1)\n",
    "            \n",
    "    test_labels = kmeans.predict(X_test)\n",
    "    for i in range(test_labels.shape[0]):\n",
    "        if test_labels[i] == 0:\n",
    "            preds_test.append(cluster_label_0)\n",
    "        else:\n",
    "            preds_test.append(cluster_label_1)\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    \n",
    "    #report for train set\n",
    "    print(\"  - train set report -\")\n",
    "    accuracy_train = accuracy_score(y_train, preds_train)\n",
    "    precision_train, recall_train, fscore_train, _ = precision_recall_fscore_support(y_train, preds_train, average='macro')\n",
    "    auc_train = roc_auc_score(lb.fit_transform(y_train),lb.transform(preds_train))\n",
    "    print(\"    accuracy =\", accuracy_train, \", precision =\", precision_train, \", recall =\", recall_train)\n",
    "    print(\"    f1 score =\", fscore_train, \", AUC =\", auc_train)\n",
    "    \n",
    "    #report for test set\n",
    "    print(\"  - test set report -\")\n",
    "    accuracy_test = accuracy_score(y_test, preds_test)\n",
    "    precision_test, recall_test, fscore_test, _ = precision_recall_fscore_support(y_test, preds_test, average='macro')\n",
    "    auc_test = roc_auc_score(lb.fit_transform(y_test),lb.transform(preds_test))\n",
    "    print(\"    accuracy =\", accuracy_test, \", precision =\", precision_test, \", recall =\", recall_test)\n",
    "    print(\"    f1 score =\", fscore_test, \", AUC =\", auc_test)\n",
    "    \n",
    "    #metrics record\n",
    "    accuracy_train_arr.append(accuracy_train)\n",
    "    precision_train_arr.append(precision_train)\n",
    "    recall_train_arr.append(recall_train)\n",
    "    fscore_train_arr.append(fscore_train)\n",
    "    auc_train_arr.append(auc_train)\n",
    "    \n",
    "    accuracy_test_arr.append(accuracy_test)\n",
    "    precision_test_arr.append(precision_test)\n",
    "    recall_test_arr.append(recall_test)\n",
    "    fscore_test_arr.append(fscore_test)\n",
    "    auc_test_arr.append(auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ae715d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised (k-means) Averages over 30 runs\n",
      "- train set -\n",
      "  average accuracy = 0.8495970695970699\n",
      "  average precision = 0.8996859345540785\n",
      "  average recall = 0.7997936016511871\n",
      "  average f1 score = 0.820946002462972\n",
      "  average AUC = 0.7997936016511871\n",
      "- test set -\n",
      "  average accuracy = 0.8540935672514619\n",
      "  average precision = 0.9049185953994311\n",
      "  average recall = 0.8024801587301589\n",
      "  average f1 score = 0.8244348383054144\n",
      "  average AUC = 0.8024801587301589\n"
     ]
    }
   ],
   "source": [
    "print(\"Unsupervised (k-means) Averages over 30 runs\")\n",
    "print(\"- train set -\")\n",
    "print(\"  average accuracy =\", sum(accuracy_train_arr)/len(accuracy_train_arr))\n",
    "print(\"  average precision =\", sum(precision_train_arr)/len(precision_train_arr))\n",
    "print(\"  average recall =\", sum(recall_train_arr)/len(recall_train_arr))\n",
    "print(\"  average f1 score =\", sum(fscore_train_arr)/len(fscore_train_arr))\n",
    "print(\"  average AUC =\", sum(auc_train_arr)/len(auc_train_arr))\n",
    "print(\"- test set -\")\n",
    "print(\"  average accuracy =\", sum(accuracy_test_arr)/len(accuracy_test_arr))\n",
    "print(\"  average precision =\", sum(precision_test_arr)/len(precision_test_arr))\n",
    "print(\"  average recall =\", sum(recall_test_arr)/len(recall_test_arr))\n",
    "print(\"  average f1 score =\", sum(fscore_test_arr)/len(fscore_test_arr))\n",
    "print(\"  average AUC =\", sum(auc_test_arr)/len(auc_test_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "84bc03a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC curve & confusion matrices: random_state set to 29 \n",
      "\n",
      "- train set - (positive label = 'M')\n",
      "[[284   1]\n",
      " [ 71  99]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test set - (positive label = 'M')\n",
      "[[72  0]\n",
      " [12 30]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('ROC curve & confusion matrices: random_state set to 29 \\n')\n",
    "print(\"- train set - (positive label = 'M')\")\n",
    "CM_train = metrics.confusion_matrix(y_train, preds_train)\n",
    "print(CM_train)\n",
    "RocCurveDisplay.from_predictions(lb.fit_transform(y_train),lb.transform(preds_train))\n",
    "plt.show()\n",
    "\n",
    "print(\"- test set - (positive label = 'M')\")\n",
    "CM_test = metrics.confusion_matrix(y_test, preds_test)\n",
    "print(CM_test)\n",
    "RocCurveDisplay.from_predictions(lb.fit_transform(y_test),lb.transform(preds_test))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb3742",
   "metadata": {},
   "source": [
    "#### 1(b)v. Spectral Clustering: Repeat 1(b)iii using spectral clustering, which is clus- tering based on kernels.3 Research what spectral clustering is. Use RBF kernel with gamma=1 or find a gamma for which the two clutsres have the same balance as the one in original data set (if the positive class has p and the negative class has n samples, the two clusters must have p and n members). Do not label data based on their proximity to cluster center, because spectral clustering may give you non-convex clusters . Instead, use fit − predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4817c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- batch, 1 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.9210526315789473 , precision = 0.9444444444444444 , recall = 0.8928571428571428\n",
      "    f1 score = 0.9105882352941177 , AUC = 0.8928571428571428\n",
      "\n",
      "---- batch, 2 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 3 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6578947368421053 , precision = 0.8243243243243243 , recall = 0.5357142857142857\n",
      "    f1 score = 0.4601092896174863 , AUC = 0.5357142857142857\n",
      "\n",
      "---- batch, 4 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 5 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 6 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6140350877192983 , precision = 0.3125 , recall = 0.4861111111111111\n",
      "    f1 score = 0.3804347826086956 , AUC = 0.4861111111111111\n",
      "\n",
      "---- batch, 7 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 8 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.7105263157894737 , precision = 0.7992056487202118 , recall = 0.6121031746031746\n",
      "    f1 score = 0.5943935309973045 , AUC = 0.6121031746031746\n",
      "\n",
      "---- batch, 9 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6241758241758242 , precision = 0.31277533039647576 , recall = 0.4982456140350877\n",
      "    f1 score = 0.3843031123139377 , AUC = 0.4982456140350877\n",
      "  - test set report -\n",
      "    accuracy = 0.6052631578947368 , precision = 0.3108108108108108 , recall = 0.4791666666666667\n",
      "    f1 score = 0.3770491803278688 , AUC = 0.4791666666666667\n",
      "\n",
      "---- batch, 10 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6754385964912281 , precision = 0.8302752293577982 , recall = 0.5595238095238095\n",
      "    f1 score = 0.5041730339720231 , AUC = 0.5595238095238095\n",
      "\n",
      "---- batch, 11 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6241758241758242 , precision = 0.31277533039647576 , recall = 0.4982456140350877\n",
      "    f1 score = 0.3843031123139377 , AUC = 0.4982456140350877\n",
      "  - test set report -\n",
      "    accuracy = 0.6140350877192983 , precision = 0.3125 , recall = 0.4861111111111111\n",
      "    f1 score = 0.3804347826086956 , AUC = 0.4861111111111111\n",
      "\n",
      "---- batch, 12 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6241758241758242 , precision = 0.31277533039647576 , recall = 0.4982456140350877\n",
      "    f1 score = 0.3843031123139377 , AUC = 0.4982456140350877\n",
      "  - test set report -\n",
      "    accuracy = 0.6228070175438597 , precision = 0.3141592920353982 , recall = 0.4930555555555556\n",
      "    f1 score = 0.3837837837837838 , AUC = 0.4930555555555556\n",
      "\n",
      "---- batch, 13 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6052631578947368 , precision = 0.3108108108108108 , recall = 0.4791666666666667\n",
      "    f1 score = 0.3770491803278688 , AUC = 0.4791666666666667\n",
      "\n",
      "---- batch, 14 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 15 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 16 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 17 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6228070175438597 , precision = 0.3141592920353982 , recall = 0.4930555555555556\n",
      "    f1 score = 0.3837837837837838 , AUC = 0.4930555555555556\n",
      "\n",
      "---- batch, 18 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6842105263157895 , precision = 0.8333333333333333 , recall = 0.5714285714285714\n",
      "    f1 score = 0.525 , AUC = 0.5714285714285714\n",
      "\n",
      "---- batch, 19 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6666666666666666 , precision = 0.8272727272727273 , recall = 0.5476190476190477\n",
      "    f1 score = 0.482560917343526 , AUC = 0.5476190476190477\n",
      "\n",
      "---- batch, 20 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.5964912280701754 , precision = 0.3090909090909091 , recall = 0.4722222222222222\n",
      "    f1 score = 0.37362637362637363 , AUC = 0.4722222222222222\n",
      "\n",
      "---- batch, 21 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 22 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 23 ----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6228070175438597 , precision = 0.3141592920353982 , recall = 0.4930555555555556\n",
      "    f1 score = 0.3837837837837838 , AUC = 0.4930555555555556\n",
      "\n",
      "---- batch, 24 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 25 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 26 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 27 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6307692307692307 , precision = 0.814569536423841 , recall = 0.5058823529411764\n",
      "    f1 score = 0.39780676876536214 , AUC = 0.5058823529411764\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 28 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 29 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n",
      "\n",
      "---- batch, 30 ----\n",
      "  - train set report -\n",
      "    accuracy = 0.6285714285714286 , precision = 0.8138766519823788 , recall = 0.5029411764705882\n",
      "    f1 score = 0.39150424550324847 , AUC = 0.5029411764705882\n",
      "  - test set report -\n",
      "    accuracy = 0.6403508771929824 , precision = 0.8185840707964602 , recall = 0.5119047619047619\n",
      "    f1 score = 0.4124450031426775 , AUC = 0.5119047619047619\n"
     ]
    }
   ],
   "source": [
    "# Unsupervised Learning: spectral clustering\n",
    "accuracy_train_arr, precision_train_arr, recall_train_arr, fscore_train_arr, auc_train_arr = [], [], [], [], [] \n",
    "accuracy_test_arr, precision_test_arr, recall_test_arr, fscore_test_arr, auc_test_arr = [], [], [], [], []\n",
    "\n",
    "for m in range(30):\n",
    "    print(\"\\n---- batch,\", m+1, \"----\")\n",
    "    \n",
    "    #train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=m, stratify=y)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train_normalized = scaler.fit_transform(X_train)\n",
    "    X_test_normalized = scaler.transform(X_test)\n",
    "    X_train_normalized, y_train = np.array(X_train_normalized), np.array(y_train)\n",
    "    X_test_normalized, y_test = np.array(X_test_normalized), np.array(y_test)\n",
    "    \n",
    "    #spectral clustering\n",
    "    clustering = SpectralClustering(n_clusters=2, affinity='rbf', gamma=1.0).fit(X_train_normalized)\n",
    "    labels_train = clustering.labels_\n",
    "    labels_test = clustering.fit_predict(X_test_normalized)\n",
    "    \n",
    "    #record preds arr according to cluster labels\n",
    "    preds_train, preds_test = [], []\n",
    "    for i in range(labels_train.shape[0]):\n",
    "        if labels_train[i] == 0:\n",
    "            preds_train.append('B')\n",
    "        else:\n",
    "            preds_train.append('M')\n",
    "            \n",
    "    for i in range(labels_test.shape[0]):\n",
    "        if labels_test[i] == 0:\n",
    "            preds_test.append('B')\n",
    "        else:\n",
    "            preds_test.append('M')\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    \n",
    "    #report for train set\n",
    "    print(\"  - train set report -\")\n",
    "    accuracy_train = accuracy_score(y_train, preds_train)\n",
    "    precision_train, recall_train, fscore_train, _ = precision_recall_fscore_support(y_train, preds_train, average='macro')\n",
    "    auc_train = roc_auc_score(lb.fit_transform(y_train),lb.transform(preds_train))\n",
    "    print(\"    accuracy =\", accuracy_train, \", precision =\", precision_train, \", recall =\", recall_train)\n",
    "    print(\"    f1 score =\", fscore_train, \", AUC =\", auc_train)\n",
    "    \n",
    "    #report for test set\n",
    "    print(\"  - test set report -\")\n",
    "    accuracy_test = accuracy_score(y_test, preds_test)\n",
    "    precision_test, recall_test, fscore_test, _ = precision_recall_fscore_support(y_test, preds_test, average='macro')\n",
    "    auc_test = roc_auc_score(lb.fit_transform(y_test),lb.transform(preds_test))\n",
    "    print(\"    accuracy =\", accuracy_test, \", precision =\", precision_test, \", recall =\", recall_test)\n",
    "    print(\"    f1 score =\", fscore_test, \", AUC =\", auc_test)\n",
    "    \n",
    "    #metrics record\n",
    "    accuracy_train_arr.append(accuracy_train)\n",
    "    precision_train_arr.append(precision_train)\n",
    "    recall_train_arr.append(recall_train)\n",
    "    fscore_train_arr.append(fscore_train)\n",
    "    auc_train_arr.append(auc_train)\n",
    "    \n",
    "    accuracy_test_arr.append(accuracy_test)\n",
    "    precision_test_arr.append(precision_test)\n",
    "    recall_test_arr.append(recall_test)\n",
    "    fscore_test_arr.append(fscore_test)\n",
    "    auc_test_arr.append(auc_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f328c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupervised (spectral clustering) Averages over 30 runs\n",
      "- train set -\n",
      "  average accuracy = 0.6293040293040292\n",
      "  average precision = 0.7641360581925682\n",
      "  average recall = 0.5040402476780187\n",
      "  average f1 score = 0.3941454779241114\n",
      "  average AUC = 0.5040402476780187\n",
      "- test set -\n",
      "  average accuracy = 0.6488304093567252\n",
      "  average precision = 0.6884797082338312\n",
      "  average recall = 0.526388888888889\n",
      "  average f1 score = 0.43719635694527165\n",
      "  average AUC = 0.526388888888889\n"
     ]
    }
   ],
   "source": [
    "print(\"Unsupervised (spectral clustering) Averages over 30 runs\")\n",
    "print(\"- train set -\")\n",
    "print(\"  average accuracy =\", sum(accuracy_train_arr)/len(accuracy_train_arr))\n",
    "print(\"  average precision =\", sum(precision_train_arr)/len(precision_train_arr))\n",
    "print(\"  average recall =\", sum(recall_train_arr)/len(recall_train_arr))\n",
    "print(\"  average f1 score =\", sum(fscore_train_arr)/len(fscore_train_arr))\n",
    "print(\"  average AUC =\", sum(auc_train_arr)/len(auc_train_arr))\n",
    "print(\"- test set -\")\n",
    "print(\"  average accuracy =\", sum(accuracy_test_arr)/len(accuracy_test_arr))\n",
    "print(\"  average precision =\", sum(precision_test_arr)/len(precision_test_arr))\n",
    "print(\"  average recall =\", sum(recall_test_arr)/len(recall_test_arr))\n",
    "print(\"  average f1 score =\", sum(fscore_test_arr)/len(fscore_test_arr))\n",
    "print(\"  average AUC =\", sum(auc_test_arr)/len(auc_test_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e625e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC curve & confusion matrices: random_state set to 29 \n",
      "\n",
      "- train set - (positive label = 'M')\n",
      "[[285   0]\n",
      " [169   1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test set - (positive label = 'M')\n",
      "[[72  0]\n",
      " [41  1]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('ROC curve & confusion matrices: random_state set to 29 \\n')\n",
    "print(\"- train set - (positive label = 'M')\")\n",
    "CM_train = metrics.confusion_matrix(y_train, preds_train)\n",
    "print(CM_train)\n",
    "RocCurveDisplay.from_predictions(lb.fit_transform(y_train),lb.transform(preds_train))\n",
    "plt.show()\n",
    "\n",
    "print(\"- test set - (positive label = 'M')\")\n",
    "CM_test = metrics.confusion_matrix(y_test, preds_test)\n",
    "print(CM_test)\n",
    "RocCurveDisplay.from_predictions(lb.fit_transform(y_test),lb.transform(preds_test))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd2607",
   "metadata": {},
   "source": [
    "#### v. One can expect that supervised learning on the full data set works better than semi-supervised learning with half of the data set labeled.One can expect that unsupervised learning underperforms in such situations. Compare the results you obtained by those methods.\n",
    "\n",
    "- Overall supervised and semi-supervised learning worked way better than unsupervised learning on this problem. supervised SVM has a better training score than semi-superbised SVM, but the two resulted in similar testing scores.\n",
    "\n",
    "- unsupervised: k-means clustering obtained acceptable results(an accuracy of 85%), while spectral clustering along performed badly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca92e38",
   "metadata": {},
   "source": [
    "### 2. Active Learning Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "861a01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = pd.read_csv('../data/data_banknote_authentication.txt',names=[1,2,3,4,'label'])\n",
    "X = bank.iloc[:,:4]\n",
    "y = bank['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=472/1372, random_state=10, stratify=y)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index)\n",
    "y_train = pd.DataFrame(y_train, index=y_train.index)\n",
    "y_test = pd.DataFrame(y_test, index=y_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1c990",
   "metadata": {},
   "source": [
    "#### 2(b)i. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the penalty parameter using 5-fold cross validation.4 Repeat this process by adding 10 other randomly selected data points to the pool, until you use all the 900 points. Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. You have implemented passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b96b96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors_df = []\n",
    "for m in range(50):\n",
    "    pool_X, pool_y = pd.DataFrame(), pd.DataFrame()\n",
    "    X_train_tmp, y_train_tmp = X_train, y_train\n",
    "    test_errors = []\n",
    "    \n",
    "    for i in range(90):\n",
    "        #randomly select 10 samples\n",
    "        sample_X = X_train_tmp.sample(n=10)\n",
    "        sample_y = y_train_tmp.loc[sample_X.index]\n",
    "    \n",
    "        #add to the pool\n",
    "        pool_X, pool_y = pd.concat([pool_X, sample_X]), pd.concat([pool_y, sample_y])\n",
    "   \n",
    "        #drop samples from remains\n",
    "        X_train_tmp, y_train_tmp = X_train_tmp.drop(sample_X.index), y_train_tmp.drop(sample_X.index)\n",
    "    \n",
    "        #svm\n",
    "        parameters_l1 = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}\n",
    "        svm = LinearSVC(penalty='l1', dual=False, max_iter=1000000)\n",
    "        cv_fold = 5\n",
    "        if(pool_y.shape[0]<20):\n",
    "            cv_fold = 2\n",
    "        clf = GridSearchCV(svm, parameters_l1, scoring='accuracy', cv=cv_fold, n_jobs=-1)\n",
    "        clf.fit(pool_X, pool_y.values.ravel())\n",
    "        best_svm = clf.best_estimator_\n",
    "        best_svm.fit(pool_X, pool_y.values.ravel())\n",
    "        test_error = 1 - best_svm.score(X_test, y_test)\n",
    "        test_errors.append(test_error)\n",
    "        #print(\"   -- test error w\",pool_y.shape[0], \"samples in the pool:\", test_error)\n",
    "    \n",
    "    test_errors_df.append(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "efc23510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.182203</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.201271</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.294492</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.192797</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.197034</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.277542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.235169</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.021186  0.031780  0.027542  0.025424  0.025424  0.027542  0.025424   \n",
       "1   0.025424  0.063559  0.046610  0.040254  0.033898  0.029661  0.029661   \n",
       "2   0.063559  0.025424  0.023305  0.023305  0.023305  0.021186  0.033898   \n",
       "3   0.091102  0.038136  0.031780  0.029661  0.027542  0.012712  0.029661   \n",
       "4   0.110169  0.029661  0.025424  0.029661  0.027542  0.027542  0.023305   \n",
       "5   0.031780  0.040254  0.029661  0.027542  0.023305  0.023305  0.029661   \n",
       "6   0.182203  0.057203  0.036017  0.036017  0.029661  0.036017  0.021186   \n",
       "7   0.148305  0.055085  0.031780  0.040254  0.029661  0.025424  0.027542   \n",
       "8   0.201271  0.029661  0.031780  0.025424  0.027542  0.025424  0.021186   \n",
       "9   0.105932  0.052966  0.038136  0.031780  0.033898  0.023305  0.042373   \n",
       "10  0.114407  0.031780  0.025424  0.025424  0.025424  0.025424  0.029661   \n",
       "11  0.103814  0.027542  0.033898  0.029661  0.027542  0.025424  0.025424   \n",
       "12  0.444915  0.031780  0.038136  0.036017  0.027542  0.031780  0.027542   \n",
       "13  0.444915  0.025424  0.033898  0.023305  0.025424  0.025424  0.027542   \n",
       "14  0.105932  0.021186  0.021186  0.021186  0.023305  0.023305  0.023305   \n",
       "15  0.029661  0.131356  0.033898  0.029661  0.029661  0.023305  0.023305   \n",
       "16  0.072034  0.040254  0.029661  0.031780  0.025424  0.027542  0.033898   \n",
       "17  0.069915  0.036017  0.027542  0.027542  0.038136  0.033898  0.029661   \n",
       "18  0.294492  0.019068  0.029661  0.023305  0.065678  0.023305  0.019068   \n",
       "19  0.033898  0.025424  0.023305  0.025424  0.025424  0.023305  0.023305   \n",
       "20  0.141949  0.110169  0.112288  0.023305  0.021186  0.055085  0.019068   \n",
       "21  0.114407  0.033898  0.019068  0.019068  0.019068  0.025424  0.016949   \n",
       "22  0.177966  0.152542  0.148305  0.118644  0.031780  0.029661  0.031780   \n",
       "23  0.114407  0.040254  0.029661  0.014831  0.019068  0.021186  0.016949   \n",
       "24  0.023305  0.021186  0.050847  0.031780  0.025424  0.023305  0.021186   \n",
       "25  0.078390  0.038136  0.029661  0.027542  0.025424  0.025424  0.025424   \n",
       "26  0.444915  0.023305  0.023305  0.023305  0.021186  0.021186  0.021186   \n",
       "27  0.114407  0.038136  0.038136  0.031780  0.029661  0.027542  0.021186   \n",
       "28  0.152542  0.118644  0.040254  0.021186  0.027542  0.027542  0.025424   \n",
       "29  0.061441  0.029661  0.025424  0.023305  0.016949  0.025424  0.023305   \n",
       "30  0.029661  0.025424  0.031780  0.042373  0.042373  0.055085  0.052966   \n",
       "31  0.008475  0.031780  0.038136  0.042373  0.025424  0.025424  0.027542   \n",
       "32  0.029661  0.152542  0.033898  0.025424  0.025424  0.029661  0.025424   \n",
       "33  0.135593  0.131356  0.101695  0.125000  0.029661  0.031780  0.029661   \n",
       "34  0.152542  0.029661  0.029661  0.021186  0.023305  0.031780  0.010593   \n",
       "35  0.029661  0.031780  0.029661  0.029661  0.029661  0.025424  0.031780   \n",
       "36  0.192797  0.061441  0.029661  0.029661  0.025424  0.021186  0.023305   \n",
       "37  0.044492  0.036017  0.029661  0.027542  0.012712  0.012712  0.012712   \n",
       "38  0.025424  0.025424  0.029661  0.027542  0.025424  0.029661  0.038136   \n",
       "39  0.197034  0.063559  0.031780  0.021186  0.008475  0.008475  0.021186   \n",
       "40  0.069915  0.074153  0.059322  0.048729  0.033898  0.025424  0.021186   \n",
       "41  0.137712  0.027542  0.033898  0.031780  0.033898  0.031780  0.014831   \n",
       "42  0.099576  0.152542  0.046610  0.029661  0.027542  0.027542  0.025424   \n",
       "43  0.277542  0.027542  0.027542  0.027542  0.025424  0.027542  0.025424   \n",
       "44  0.141949  0.103814  0.144068  0.040254  0.010593  0.023305  0.023305   \n",
       "45  0.235169  0.029661  0.025424  0.029661  0.023305  0.016949  0.016949   \n",
       "46  0.131356  0.152542  0.152542  0.023305  0.023305  0.023305  0.023305   \n",
       "47  0.131356  0.029661  0.023305  0.023305  0.025424  0.025424  0.023305   \n",
       "48  0.137712  0.044492  0.025424  0.025424  0.025424  0.025424  0.027542   \n",
       "49  0.055085  0.038136  0.046610  0.027542  0.031780  0.052966  0.040254   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.023305  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "1   0.021186  0.025424  0.025424  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "2   0.025424  0.031780  0.031780  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "3   0.025424  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "4   0.031780  0.025424  0.029661  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "5   0.023305  0.029661  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "6   0.021186  0.021186  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "7   0.029661  0.029661  0.029661  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "8   0.021186  0.023305  0.029661  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "9   0.031780  0.025424  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "10  0.029661  0.029661  0.023305  ...  0.012712  0.010593  0.010593  0.010593   \n",
       "11  0.025424  0.025424  0.040254  ...  0.012712  0.012712  0.012712  0.010593   \n",
       "12  0.023305  0.023305  0.023305  ...  0.012712  0.010593  0.010593  0.012712   \n",
       "13  0.016949  0.014831  0.014831  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "14  0.027542  0.029661  0.029661  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "15  0.023305  0.025424  0.025424  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "16  0.046610  0.040254  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "17  0.027542  0.014831  0.019068  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "18  0.019068  0.019068  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "19  0.023305  0.021186  0.021186  ...  0.012712  0.012712  0.012712  0.010593   \n",
       "20  0.023305  0.016949  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "21  0.010593  0.016949  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "22  0.031780  0.027542  0.029661  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "23  0.023305  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "24  0.021186  0.021186  0.027542  ...  0.012712  0.012712  0.012712  0.010593   \n",
       "25  0.031780  0.025424  0.027542  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "26  0.023305  0.016949  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "27  0.021186  0.021186  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "28  0.021186  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "29  0.016949  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "30  0.052966  0.021186  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "31  0.019068  0.027542  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "32  0.029661  0.021186  0.027542  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "33  0.025424  0.023305  0.016949  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "34  0.029661  0.008475  0.008475  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "35  0.031780  0.031780  0.031780  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "36  0.025424  0.021186  0.031780  ...  0.010593  0.012712  0.012712  0.012712   \n",
       "37  0.029661  0.027542  0.010593  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "38  0.021186  0.021186  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "39  0.010593  0.008475  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "40  0.021186  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "41  0.023305  0.023305  0.014831  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "42  0.027542  0.027542  0.027542  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "43  0.023305  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "44  0.014831  0.010593  0.010593  ...  0.010593  0.010593  0.010593  0.012712   \n",
       "45  0.031780  0.016949  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "46  0.023305  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "47  0.023305  0.023305  0.023305  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "48  0.033898  0.021186  0.021186  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "49  0.027542  0.023305  0.033898  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "1   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "2   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "3   0.010593  0.010593  0.010593  0.012712  0.012712  0.012712  \n",
       "4   0.010593  0.010593  0.012712  0.012712  0.012712  0.012712  \n",
       "5   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "6   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "7   0.010593  0.012712  0.012712  0.012712  0.010593  0.010593  \n",
       "8   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "9   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "10  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "11  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "12  0.012712  0.012712  0.012712  0.012712  0.010593  0.010593  \n",
       "13  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "14  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "15  0.010593  0.010593  0.012712  0.012712  0.012712  0.012712  \n",
       "16  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "17  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "18  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "19  0.010593  0.010593  0.010593  0.012712  0.012712  0.010593  \n",
       "20  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "21  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "22  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "23  0.012712  0.012712  0.010593  0.010593  0.010593  0.010593  \n",
       "24  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "25  0.010593  0.010593  0.012712  0.010593  0.010593  0.010593  \n",
       "26  0.010593  0.008475  0.008475  0.012712  0.010593  0.010593  \n",
       "27  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "28  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "29  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "30  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "31  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "32  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "33  0.012712  0.010593  0.010593  0.010593  0.012712  0.012712  \n",
       "34  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "35  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "36  0.012712  0.012712  0.010593  0.010593  0.010593  0.010593  \n",
       "37  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "38  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "39  0.010593  0.010593  0.010593  0.010593  0.014831  0.010593  \n",
       "40  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "41  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "42  0.012712  0.012712  0.010593  0.012712  0.012712  0.012712  \n",
       "43  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "44  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "45  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "46  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "47  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "48  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "49  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passive_test_errors = pd.DataFrame(test_errors_df)\n",
    "passive_test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71ee48",
   "metadata": {},
   "source": [
    "#### 2(b)ii. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the parameters of the SVM with 5-fold cross validation. Choose the 10 closest data points in the training set to the hyperplane of the SVM6 and add them to the pool. Do not replace the samples back into the training set. Train a new SVM using the pool. Repeat this process until all training data is used. You will have 90 SVMs that were trained using 10, 20, 30,..., 900 data points and their 90 test errors. You have implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "52dc21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_errors_df = []\n",
    "for m in range(50):\n",
    "    pool_X, pool_y = pd.DataFrame(), pd.DataFrame()\n",
    "    X_train_tmp, y_train_tmp = X_train, y_train\n",
    "    test_errors = []\n",
    "    \n",
    "    for i in range(90):\n",
    "        #select 10 samples\n",
    "        if i==0:\n",
    "            sample_X = X_train_tmp.sample(n=10,random_state=m)\n",
    "            sample_y = y_train_tmp.loc[sample_X.index]\n",
    "        else:\n",
    "            distances = best_svm.decision_function(X_train_tmp)\n",
    "            closest_indexes = np.argsort(abs(distances))[:10]\n",
    "            sample_X = X_train_tmp.iloc[closest_indexes]\n",
    "            sample_y = y_train_tmp.iloc[closest_indexes]\n",
    "            \n",
    "        #add to the pool\n",
    "        pool_X, pool_y = pd.concat([pool_X, sample_X]), pd.concat([pool_y, sample_y])\n",
    "   \n",
    "        #drop samples from remains\n",
    "        X_train_tmp, y_train_tmp = X_train_tmp.drop(sample_X.index), y_train_tmp.drop(sample_X.index)\n",
    "    \n",
    "        #svm\n",
    "        parameters_l1 = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]}\n",
    "        svm = LinearSVC(penalty='l1', dual=False, max_iter=1000000)\n",
    "        cv_fold = 5\n",
    "        if(pool_y.shape[0]<30):\n",
    "            cv_fold = 2\n",
    "        clf = GridSearchCV(svm, parameters_l1, scoring='accuracy', cv=cv_fold, n_jobs=-1)\n",
    "        clf.fit(pool_X, pool_y.values.ravel())\n",
    "        best_svm = clf.best_estimator_\n",
    "        best_svm.fit(pool_X, pool_y.values.ravel())\n",
    "        test_error = 1 - best_svm.score(X_test, y_test)\n",
    "        test_errors.append(test_error)\n",
    "        #print(\"   -- test error w\",pool_y.shape[0], \"samples in the pool:\", test_error)\n",
    "    \n",
    "    test_errors_df.append(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "73aaac2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.194915</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.294492</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.224576</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.150424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.137712</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.031780  0.444915  0.023305  0.014831  0.012712  0.010593  0.010593   \n",
       "1   0.036017  0.021186  0.023305  0.016949  0.023305  0.010593  0.012712   \n",
       "2   0.194915  0.040254  0.031780  0.010593  0.010593  0.010593  0.010593   \n",
       "3   0.444915  0.038136  0.014831  0.010593  0.010593  0.012712  0.010593   \n",
       "4   0.027542  0.008475  0.010593  0.012712  0.010593  0.010593  0.010593   \n",
       "5   0.139831  0.137712  0.078390  0.029661  0.023305  0.014831  0.012712   \n",
       "6   0.131356  0.152542  0.036017  0.023305  0.014831  0.008475  0.008475   \n",
       "7   0.082627  0.444915  0.027542  0.027542  0.010593  0.014831  0.010593   \n",
       "8   0.137712  0.048729  0.025424  0.010593  0.023305  0.008475  0.010593   \n",
       "9   0.169492  0.088983  0.040254  0.050847  0.010593  0.008475  0.008475   \n",
       "10  0.152542  0.061441  0.029661  0.014831  0.025424  0.025424  0.012712   \n",
       "11  0.038136  0.014831  0.023305  0.010593  0.006356  0.010593  0.010593   \n",
       "12  0.031780  0.023305  0.008475  0.008475  0.010593  0.008475  0.012712   \n",
       "13  0.061441  0.025424  0.046610  0.021186  0.016949  0.014831  0.010593   \n",
       "14  0.444915  0.069915  0.023305  0.014831  0.014831  0.010593  0.010593   \n",
       "15  0.133475  0.152542  0.027542  0.033898  0.008475  0.008475  0.010593   \n",
       "16  0.082627  0.033898  0.029661  0.019068  0.016949  0.012712  0.010593   \n",
       "17  0.033898  0.148305  0.010593  0.016949  0.012712  0.010593  0.008475   \n",
       "18  0.440678  0.158898  0.029661  0.025424  0.012712  0.014831  0.010593   \n",
       "19  0.150424  0.027542  0.014831  0.044492  0.023305  0.014831  0.012712   \n",
       "20  0.036017  0.444915  0.095339  0.048729  0.038136  0.023305  0.023305   \n",
       "21  0.027542  0.046610  0.008475  0.010593  0.012712  0.012712  0.010593   \n",
       "22  0.080508  0.044492  0.055085  0.012712  0.010593  0.012712  0.010593   \n",
       "23  0.294492  0.029661  0.048729  0.027542  0.019068  0.016949  0.014831   \n",
       "24  0.141949  0.072034  0.021186  0.025424  0.010593  0.010593  0.010593   \n",
       "25  0.110169  0.025424  0.010593  0.008475  0.008475  0.010593  0.010593   \n",
       "26  0.061441  0.027542  0.010593  0.023305  0.010593  0.010593  0.008475   \n",
       "27  0.224576  0.444915  0.110169  0.038136  0.025424  0.021186  0.023305   \n",
       "28  0.084746  0.099576  0.021186  0.010593  0.031780  0.008475  0.010593   \n",
       "29  0.152542  0.036017  0.014831  0.019068  0.016949  0.012712  0.012712   \n",
       "30  0.036017  0.042373  0.025424  0.010593  0.012712  0.010593  0.010593   \n",
       "31  0.065678  0.091102  0.040254  0.008475  0.033898  0.019068  0.010593   \n",
       "32  0.139831  0.114407  0.061441  0.016949  0.014831  0.008475  0.012712   \n",
       "33  0.444915  0.078390  0.144068  0.069915  0.046610  0.023305  0.016949   \n",
       "34  0.122881  0.012712  0.021186  0.010593  0.012712  0.010593  0.010593   \n",
       "35  0.131356  0.010593  0.008475  0.036017  0.019068  0.008475  0.012712   \n",
       "36  0.150424  0.031780  0.052966  0.010593  0.008475  0.008475  0.012712   \n",
       "37  0.027542  0.014831  0.014831  0.021186  0.008475  0.010593  0.010593   \n",
       "38  0.029661  0.065678  0.021186  0.010593  0.010593  0.012712  0.010593   \n",
       "39  0.061441  0.016949  0.010593  0.010593  0.010593  0.012712  0.012712   \n",
       "40  0.120763  0.023305  0.023305  0.010593  0.012712  0.008475  0.010593   \n",
       "41  0.059322  0.025424  0.016949  0.010593  0.021186  0.010593  0.008475   \n",
       "42  0.084746  0.023305  0.023305  0.014831  0.008475  0.008475  0.010593   \n",
       "43  0.088983  0.173729  0.023305  0.016949  0.010593  0.014831  0.010593   \n",
       "44  0.052966  0.012712  0.021186  0.012712  0.014831  0.012712  0.010593   \n",
       "45  0.074153  0.148305  0.082627  0.014831  0.008475  0.014831  0.012712   \n",
       "46  0.031780  0.099576  0.046610  0.027542  0.016949  0.008475  0.008475   \n",
       "47  0.029661  0.444915  0.025424  0.010593  0.008475  0.008475  0.010593   \n",
       "48  0.137712  0.044492  0.016949  0.012712  0.014831  0.008475  0.010593   \n",
       "49  0.135593  0.050847  0.052966  0.025424  0.021186  0.012712  0.016949   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.010593  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "1   0.012712  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "2   0.010593  0.010593  0.008475  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "3   0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "4   0.012712  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "5   0.012712  0.010593  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "6   0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "7   0.012712  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "8   0.010593  0.010593  0.008475  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "9   0.010593  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "10  0.010593  0.010593  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "11  0.010593  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "12  0.012712  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "13  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "14  0.012712  0.008475  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "15  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "16  0.023305  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "17  0.012712  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "18  0.010593  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "19  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "20  0.014831  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "21  0.010593  0.010593  0.008475  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "22  0.010593  0.010593  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "23  0.012712  0.010593  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "24  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "25  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "26  0.012712  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "27  0.010593  0.014831  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "28  0.008475  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "29  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "30  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "31  0.012712  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "32  0.012712  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "33  0.012712  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "34  0.012712  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "35  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "36  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "37  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "38  0.008475  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "39  0.008475  0.008475  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "40  0.010593  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "41  0.010593  0.010593  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "42  0.012712  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "43  0.012712  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "44  0.010593  0.012712  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "45  0.008475  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "46  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "47  0.012712  0.012712  0.012712  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "48  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "49  0.010593  0.010593  0.010593  ...  0.012712  0.012712  0.012712  0.012712   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.010593  0.010593  0.012712  0.012712  0.012712  0.010593  \n",
       "1   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "2   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "3   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "4   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "5   0.012712  0.012712  0.012712  0.010593  0.010593  0.012712  \n",
       "6   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "7   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "8   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "9   0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "10  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "11  0.012712  0.012712  0.010593  0.012712  0.012712  0.012712  \n",
       "12  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "13  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "14  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "15  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "16  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "17  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "18  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "19  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "20  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "21  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "22  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "23  0.012712  0.010593  0.012712  0.012712  0.012712  0.012712  \n",
       "24  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "25  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "26  0.010593  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "27  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "28  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "29  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "30  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "31  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "32  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "33  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "34  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "35  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "36  0.012712  0.012712  0.012712  0.012712  0.012712  0.010593  \n",
       "37  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "38  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "39  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "40  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "41  0.012712  0.010593  0.010593  0.010593  0.012712  0.010593  \n",
       "42  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "43  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "44  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "45  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "46  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "47  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "48  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "49  0.012712  0.012712  0.012712  0.012712  0.012712  0.012712  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_test_errors = pd.DataFrame(test_errors_df)\n",
    "active_test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be698bb",
   "metadata": {},
   "source": [
    "#### 2(c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i and 2(b)ii. By doing so, you are performing a Monte Carlo simulation. Plot average test error versus number of training instances for both active and passive learners on the same figure and report your conclusions. Here, you are actually obtaining a learning curve by Monte-Carlo simulation.\n",
    "\n",
    "- Passive learning started to work better than active learning when the size of the train set is very small.\n",
    "- However, error rates of active learning dropped faster while the training size increased.\n",
    "- They ended up equally well when all samples were added to the training set.\n",
    "- It is pointless to select 10 closest samples when the training size is too small, but we may consider combine the two approach together: Firstly PASSSIVE, then ACTIVE when the training size growed to certain threshold.\n",
    "- (note: possible reason for the bump on the curve of active learning might be those poorly selected initial samples. They might not included both classes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d53df4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i*10 for i in range(1,91)], passive_test_errors.mean(),label='Passive')\n",
    "plt.plot([i*10 for i in range(1,91)], active_test_errors.mean(), label='Active')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
